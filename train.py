import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from tqdm import tqdm
from main import (
    GENERATORS,
    DISCRIMINATORS,
    NUM_EPOCHS,
    L1_LAMBDA,
    device
)


def train(
    train_loader: torch.utils.data.DataLoader,
    val_loader: torch.utils.data.DataLoader,
    generator: str,
    discriminator: str,
):
    # Generator.
    generator = GENERATORS[generator](3, 3)
    generator.to(device)
    generator.train()

    # Discriminator.
    discriminator = DISCRIMINATORS[discriminator](3)
    discriminator.to(device)
    discriminator.train()

    # Loss fuctions. BCE for discriminator and L1 for generator.
    bce_loss = nn.BCELoss().to(device)
    l1_loss = nn.L1Loss().to(device)

    # Optimizer.
    g_optimizer = optim.Adam(generator.parameters())
    d_optimizer = optim.Adam(discriminator.parameters())

    # Get image size so all images have the same size. It is also used in
    # separating the input from the label image, since they are placed
    # horizontally next to each other. We assume the images are square.
    initial_datapoint = next(iter(train_loader))[0]
    img_size = initial_datapoint.shape[2]

    for epoch in range(NUM_EPOCHS):
        d_losses = []
        g_losses = []
        print(f"Epoch {epoch}")
        for xy_concat, _ in tqdm(train_loader):
            # Discriminator training.
            discriminator.zero_grad()

            # Separate input and label image. Left = input, right = label.
            x = xy_concat[:, :, :, :img_size].to(device)
            y = xy_concat[:, :, :, img_size:].to(device)

            x, y = Variable(x).to(device), Variable(y).to(device)
            y_pred = generator(x)

            # The discriminator should predict all ones for "real" images, i.e.
            # images from the dataset.
            d_real_result = discriminator(x, y)
            d_real_loss = bce_loss(
                d_real_result,
                Variable(torch.ones(d_real_result.shape)).to(device),
            )

            # The discriminator should predict all zeros for "fake" images,
            # i.e. images generated by the generator.
            d_fake_result = discriminator(x, y_pred)
            d_fake_loss = bce_loss(
                d_fake_result,
                Variable(torch.zeros(d_fake_result.shape)).to(device),
            )

            # Loss of discriminator is the average of the BCE losses of "real"
            # and "fake" data.
            d_loss = (d_real_loss + d_fake_loss) * 0.5
            d_loss.backward()
            d_optimizer.step()

            d_losses.append(d_loss.item())

            # Generator training.
            generator.zero_grad()

            y_pred = generator(x)
            d_result = discriminator(x, y_pred).squeeze()

            # The training loss of the generator is a combination of how well
            # it was able to fool the discriminator (BCE) and how much it
            # resembles the real image (L1).
            g_loss = bce_loss(
                d_result,
                Variable(torch.ones(d_result.size(), device=device))
            ) + L1_LAMBDA * l1_loss(y_pred, y)
            g_loss.backward()
            g_optimizer.step()

            g_losses.append(g_loss.item())

        print("[%d/%d] - d_loss: %.3f, g_loss: %.3f" % (
            epoch + 1,
            NUM_EPOCHS,
            torch.mean(torch.FloatTensor(d_losses)),
            torch.mean(torch.FloatTensor(g_losses)),
        ))
